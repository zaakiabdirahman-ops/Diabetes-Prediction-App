# -*- coding: utf-8 -*-
"""Diabete_prediction_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ynOSfCLiusat_mayZuqV_MB8PXlJ1Bb7
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

import pickle
filename = 'tuned_diabetes_model.pkl'
pickle.dump(gbc_tuned, open(filename, 'wb'))

importances = gbc_tuned.feature_importances_
feature_names = x_train_resampled.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances (Tuned GBC)")
plt.bar(range(x_train_resampled.shape[1]), importances[indices], align="center")
plt.xticks(range(x_train_resampled.shape[1]), feature_names[indices], rotation=90)
plt.xlim([-1, x_train_resampled.shape[1]])
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve
import matplotlib.pyplot as plt

# 3. Generate and plot the confusion matrix
cm = confusion_matrix(y_test, y_pred_tuned)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gbc_tuned.classes_)
disp.plot(cmap='Blues_r', values_format='d', colorbar=True)
plt.title('Confusion Matrix (Tuned GBC)')
plt.show()

# 4. Calculate the predicted probabilities for the positive class
y_pred_proba_tuned = gbc_tuned.predict_proba(x_test)[:, 1]

# 5. Calculate and plot the ROC curve and AUC score
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_tuned)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Tuned GBC)')
plt.legend(loc="lower right")
plt.show()

# 6. Calculate and plot the Precision-Recall curve
precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_tuned)
plt.figure()
plt.plot(recall_curve, precision_curve, color='pink', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (Tuned GBC)')
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# 1. Make predictions on the test set
y_pred_tuned = gbc_tuned.predict(x_test)

# 2. Calculate and print the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_tuned)
recall = recall_score(y_test, y_pred_tuned)
f1 = f1_score(y_test, y_pred_tuned)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

diabete=pd.read_csv('/content/diabetes_prediction_dataset.csv')

diabete.head()

diabete.shape

diabete.columns

diabete.info()

diabete.describe()

diabete.isnull().sum()

diabete.duplicated().sum()

diabete['smoking_history'].value_counts()

def add_counts(ax):
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                    textcoords='offset points')

# Set up the matplotlib figure
fig, axes = plt.subplots(3, 2, figsize=(15, 15))

# Plot gender grouped by diabetes
ax = sns.countplot(ax=axes[0, 0], x='gender', hue='diabetes', data=diabete)
axes[0, 0].set_title('Gender grouped by Diabetes')
add_counts(ax)

# Plot hypertension grouped by diabetes
ax = sns.countplot(ax=axes[0, 1], x='hypertension', hue='diabetes', data=diabete)
axes[0, 1].set_title('Hypertension grouped by Diabetes')
add_counts(ax)

# Plot heart disease grouped by diabetes
ax = sns.countplot(ax=axes[1, 0], x='heart_disease', hue='diabetes', data=diabete)
axes[1, 0].set_title('Heart Disease grouped by Diabetes')
add_counts(ax)

# Plot smoking history grouped by diabetes
ax = sns.countplot(ax=axes[1, 1], x='smoking_history', hue='diabetes', data=diabete)
axes[1, 1].set_title('Smoking History grouped by Diabetes')
add_counts(ax)

# Plot diabetes
ax = sns.countplot(ax=axes[2, 0], x='diabetes', data=diabete)
axes[2, 0].set_title('Diabetes Count')
add_counts(ax)

# Create pie plot for diabetes
diabetes_counts = diabete['diabetes'].value_counts()
axes[2, 1].pie(diabetes_counts, labels=diabetes_counts.index, autopct='%1.1f%%', startangle=90)
axes[2, 1].set_title('Diabetes Distribution')
axes[2, 1].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
axes[2, 1].legend(title='Diabetes:', loc='upper right')
# Adjust the layout
plt.tight_layout()

# Show the plots
plt.show()

# Calculate the proportion of diabetes within each gender group
gender_diabetes_counts = diabete.groupby(['gender', 'diabetes'])['diabetes'].count().unstack()
gender_diabetes_counts['total'] = gender_diabetes_counts.sum(axis=1)
gender_diabetes_counts['proportion_with_diabetes'] = gender_diabetes_counts[1] / gender_diabetes_counts['total']

# Display the results
print(gender_diabetes_counts)

# Print a summary
print("\nInterpretation:")
print("The countplot shows that there is a slightly higher proportion of females with diabetes compared to males.")

cross_table = pd.crosstab(diabete['diabetes'], diabete['smoking_history'])

# Create subplots
fig, ax = plt.subplots(1, 2, figsize=(20, 8))

# Plotting the cross table as a heatmap
sns.heatmap(cross_table, cmap='YlOrRd', annot=True, fmt='d', linewidths=0.5, linecolor='black', ax=ax[0])
ax[0].set_title('Diabetes and Smoking History (Heatmap)')
ax[0].set_xlabel('Smoking History')
ax[0].set_ylabel('Diabetes')

# Plotting the cross table with separate bars for smoking history
cross_table.plot(kind='bar', stacked=False, ax=ax[1], color=plt.cm.Paired.colors)
ax[1].set_title('Diabetes and Smoking History (Bar Plot)')
ax[1].set_xlabel('Diabetes')
ax[1].set_ylabel('Count')
ax[1].legend(title='Smoking History', bbox_to_anchor=(1.05, 1), loc='upper left')

# Annotate bars with their values
for container in ax[1].containers:
    ax[1].bar_label(container)

plt.tight_layout()
plt.show()

import sklearn
from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

for i in diabete.columns:
  if diabete[i].dtype=='object':
    diabete[i]=le.fit_transform(diabete[i])

diabete.head()

diabete['smoking_history'].value_counts()

corr_matrix=diabete.corr()

plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix,annot=True,cmap='icefire',fmt=".2f",linewidths=2,linecolor='black')
plt.title('Correlation Matrix')
plt.show()

diabete['bmi'].max()

diabete['bmi'].min()

diabete['HbA1c_level'].max()

diabete['HbA1c_level'].min()

diabete.drop(['gender','smoking_history'],axis=1,inplace=True)

diabete.fillna(0,inplace=True)

X=diabete.drop('diabetes',axis=1)
Y=diabete['diabetes']

diabete.columns

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
models=[
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    SVC(),
    GradientBoostingClassifier(),
    KNeighborsClassifier()
]
for model in models:
  model.fit(x_train,y_train)
  y_pred=model.predict(x_test)
  acc=accuracy_score(y_test,y_pred)
  print(f"Model: {model.__class__.__name__}")
  print(f"ACC: {acc}")
  print()

from sklearn.ensemble import GradientBoostingClassifier
gbc=GradientBoostingClassifier()
gbc.fit(x_train,y_train)
y_pred_gbc=gbc.predict(x_test)

from sklearn.metrics import precision_score, recall_score
precision = precision_score(y_test, y_pred_gbc)
recall = recall_score(y_test, y_pred_gbc)
print('Precision: {} / Recall: {} / Accuracy: {}'.format(
    round(precision, 3), round(recall, 3), round((y_pred_gbc==y_test).sum()/len(y_pred_gbc), 3)))

from sklearn.metrics import classification_report
y_pred = gbc.predict(x_test)
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred_rf=rf.predict(x_test)

from sklearn.metrics import precision_score, recall_score
precision = precision_score(y_test, y_pred_rf)
recall = recall_score(y_test, y_pred_rf)
print('Precision: {} / Recall: {} / Accuracy: {}'.format(
    round(precision, 3), round(recall, 3), round((y_pred_rf==y_test).sum()/len(y_pred_rf), 3)))

Output=pd.DataFrame({'Actual':y_test,'Predicted':y_pred_gbc})
Output

wrong_pred=Output[Output['Actual']!=Output['Predicted']]
len(wrong_pred)

prediction=gbc.predict([[1,1,0,35.5,7.2,210]])
print(prediction)

diabete.head()

prediction=gbc.predict([[23,0,0,16.8,4.5,80]])
print(prediction)

diabete.columns

diabete.head(50)

diabete.loc[diabete['diabetes']==1].value_counts().sum()

import pickle
filename='diabetes_model.pkl'
pickle.dump(gbc,open(filename,'wb'))

# prompt: generate confusion matrix with plot

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred_gbc)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gbc.classes_)
disp.plot(cmap='Blues_r', values_format='d',colorbar=True)
plt.title('Confusion Matrix')
plt.show()

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming 'gbc' is your trained Gradient Boosting Classifier
# Select a single tree from the ensemble (e.g., the first tree)
tree_index = 2
estimator = gbc.estimators_[tree_index][0]

# Plot the tree
plt.figure(figsize=(15, 10))
plot_tree(estimator,
          feature_names=x_train.columns,
          class_names=['No Diabetes', 'Diabetes'],
          filled=True,
          rounded=True,
          fontsize=8)
plt.show()

from sklearn.calibration import calibration_curve

# Assuming 'gbc' is your trained Gradient Boosting Classifier and 'x_test', 'y_test' are your test data
y_pred_proba_gb = gbc.predict_proba(x_test)[:, 1]

# Calculate calibration curve
prob_true, prob_pred = calibration_curve(y_test, y_pred_proba_gb, n_bins=10)

# Plot calibration curve
plt.figure()
plt.plot(prob_pred, prob_true, marker='o', label='Gradient Boosting')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.title('Calibration Curve')
plt.legend()
plt.show()

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
y_pred_proba_gb = gbc.predict_proba(x_test)[:, 1]

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_gb)
plt.figure()
plt.plot(recall, precision, color='pink', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

importances = gbc.feature_importances_
feature_names = x_train.columns
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(x_train.shape[1]), importances[indices], align="center")
plt.xticks(range(x_train.shape[1]), feature_names[indices], rotation=90)
plt.xlim([-1, x_train.shape[1]])
plt.tight_layout()
plt.show()

# prompt: how can be plotted GBC tree

# Assuming 'gbc' is your trained Gradient Boosting Classifier
# Select a single tree from the ensemble (e.g., the first tree)
tree_index = 0  # You can change this index to visualize different trees
estimator = gbc.estimators_[tree_index][0]

# Plot the tree
plt.figure(figsize=(15, 10))
plot_tree(estimator,
          feature_names=x_train.columns,
          class_names=['No Diabetes', 'Diabetes'],  # Adjust class names if needed
          filled=True,
          rounded=True,
          fontsize=8)
plt.show()

# prompt: generate ROC curve

from sklearn.metrics import roc_curve, auc

# Assuming 'gbc' is your trained Gradient Boosting Classifier and 'x_test', 'y_test' are your test data
y_pred_proba_gb = gbc.predict_proba(x_test)[:, 1]

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_gb)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Decision Tree Classifier
dtc = DecisionTreeClassifier(random_state=42)
dtc.fit(x_train, y_train)
y_pred_dtc = dtc.predict(x_test)
print("Decision Tree Classifier Accuracy:", accuracy_score(y_test, y_pred_dtc))
print(classification_report(y_test, y_pred_dtc))

# Support Vector Machine (SVM) Classifier
svc = SVC(kernel='linear', random_state=42)
svc.fit(x_train, y_train)
y_pred_svc = svc.predict(x_test)
print("SVM Classifier Accuracy:", accuracy_score(y_test, y_pred_svc))
print(classification_report(y_test, y_pred_svc))

# K-Nearest Neighbors (KNN) Classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)
y_pred_knn = knn.predict(x_test)
print("KNN Classifier Accuracy:", accuracy_score(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

# Random Forest Classifier
rfc = RandomForestClassifier(random_state=42)
rfc.fit(x_train, y_train)
y_pred_rfc = rfc.predict(x_test)
print("Random Forest Classifier Accuracy:", accuracy_score(y_test, y_pred_rfc))
print(classification_report(y_test, y_pred_rfc))



"""# Task
Analyze the provided dataset, address class imbalance, tune the hyperparameters of a Gradient Boosting Classifier, train the model, evaluate its performance using various metrics and visualizations, and identify important features.

## Load the new dataset

### Subtask:
Load the newly uploaded dataset into a pandas DataFrame.

**Reasoning**:
Load the dataset into a pandas DataFrame and display the first few rows to confirm the load.
"""

diabete = pd.read_csv('/content/diabetes_prediction_dataset.csv')
diabete.head()

"""**Reasoning**:
The previous command failed because the pandas library was not imported. I will import pandas and then load the data into a pandas DataFrame and display the first few rows to confirm the load.


"""

import pandas as pd
diabete = pd.read_csv('/content/diabetes_prediction_dataset.csv')
diabete.head()

"""## Handle duplicates and missing values

### Subtask:
Check for and handle any duplicate rows or missing values in the new dataset.

**Reasoning**:
Check for and report the number of duplicate rows and missing values in the dataframe, as per the subtask instructions. Based on previous execution, there are no missing values but duplicates are present.
"""

print("Number of duplicate rows:", diabete.duplicated().sum())
print("Missing values per column:\n", diabete.isnull().sum())

"""**Reasoning**:
As duplicates were found and no missing values were detected, the next step is to remove the duplicate rows from the dataframe to ensure data cleanliness as per the subtask instructions.


"""

diabete.drop_duplicates(inplace=True)
print("Number of duplicate rows after removal:", diabete.duplicated().sum())

"""## Address class imbalance

### Subtask:
Implement a technique like SMOTE to address the class imbalance in the training data.

**Reasoning**:
Import the SMOTE class and apply it to the training data to address class imbalance.
"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

print("Value counts of the target variable in the original training data:")
print(y_train.value_counts())
print("\nValue counts of the target variable in the resampled training data:")
print(y_train_resampled.value_counts())

"""**Reasoning**:
The variables `x_train` and `y_train` were not defined in the current session, so I need to re-run the code that splits the data into training and testing sets before applying SMOTE.


"""

from sklearn.model_selection import train_test_split
X=diabete.drop('diabetes',axis=1)
Y=diabete['diabetes']
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

print("Value counts of the target variable in the original training data:")
print(y_train.value_counts())
print("\nValue counts of the target variable in the resampled training data:")
print(y_train_resampled.value_counts())

"""**Reasoning**:
The error indicates that there are still non-numeric columns in the DataFrame that need to be encoded before applying SMOTE. I need to re-apply label encoding to the object type columns in the `diabete` DataFrame before splitting the data and applying SMOTE.


"""

le = LabelEncoder()
for i in diabete.columns:
  if diabete[i].dtype=='object':
    diabete[i]=le.fit_transform(diabete[i])

X=diabete.drop('diabetes',axis=1)
Y=diabete['diabetes']
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

print("Value counts of the target variable in the original training data:")
print(y_train.value_counts())
print("\nValue counts of the target variable in the resampled training data:")
print(y_train_resampled.value_counts())

"""**Reasoning**:
The error indicates that `LabelEncoder` was not imported. I need to import `LabelEncoder` before using it to encode the categorical columns.


"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for i in diabete.columns:
  if diabete[i].dtype=='object':
    diabete[i]=le.fit_transform(diabete[i])

X=diabete.drop('diabetes',axis=1)
Y=diabete['diabetes']
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

smote = SMOTE(random_state=42)
x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)

print("Value counts of the target variable in the original training data:")
print(y_train.value_counts())
print("\nValue counts of the target variable in the resampled training data:")
print(y_train_resampled.value_counts())

"""## Hyperparameter tuning

### Subtask:
Perform hyperparameter tuning for the Gradient Boosting Classifier using GridSearchCV or RandomizedSearchCV to find the best parameters.

**Reasoning**:
Perform hyperparameter tuning for the Gradient Boosting Classifier using GridSearchCV on the resampled training data.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 4]
}

gbc = GradientBoostingClassifier(random_state=42)
grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)

grid_search.fit(x_train_resampled, y_train_resampled)

print("Best parameters found: ", grid_search.best_params_)

"""## Train the model

### Subtask:
Train the Gradient Boosting Classifier with the best hyperparameters on the balanced training data.

**Reasoning**:
Retrieve the best hyperparameters, instantiate a Gradient Boosting Classifier with these parameters, and train the model on the resampled training data.
"""

best_params = grid_search.best_params_
gbc_tuned = GradientBoostingClassifier(**best_params, random_state=42)
gbc_tuned.fit(x_train_resampled, y_train_resampled)

"""## Evaluate the model

### Subtask:
Evaluate the trained model using appropriate metrics such as precision, recall, F1-score, and AUC, and visualize the results with a confusion matrix, ROC curve, and precision-recall curve.

**Reasoning**:
Make predictions, calculate evaluation metrics, and generate plots for confusion matrix, ROC curve, and Precision-Recall curve to evaluate the tuned Gradient Boosting Classifier.
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve
import matplotlib.pyplot as plt

# 1. Make predictions on the test set
y_pred_tuned = gbc_tuned.predict(x_test)

# 2. Calculate and print the precision, recall, and F1-score
precision = precision_score(y_test, y_pred_tuned)
recall = recall_score(y_test, y_pred_tuned)
f1 = f1_score(y_test, y_pred_tuned)

print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# 3. Generate and plot the confusion matrix
cm = confusion_matrix(y_test, y_pred_tuned)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gbc_tuned.classes_)
disp.plot(cmap='Blues_r', values_format='d', colorbar=True)
plt.title('Confusion Matrix (Tuned GBC)')
plt.show()

# 4. Calculate the predicted probabilities for the positive class
y_pred_proba_tuned = gbc_tuned.predict_proba(x_test)[:, 1]

# 5. Calculate and plot the ROC curve and AUC score
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_tuned)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Tuned GBC)')
plt.legend(loc="lower right")
plt.show()

# 6. Calculate and plot the Precision-Recall curve
precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_tuned)
plt.figure()
plt.plot(recall_curve, precision_curve, color='pink', lw=2)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (Tuned GBC)')
plt.show()

"""## Feature importance

### Subtask:
Plot feature importances for the trained model.

**Reasoning**:
Retrieve feature importances, sort them, and plot a bar chart to visualize the feature importances of the tuned Gradient Boosting Classifier.
"""

importances = gbc_tuned.feature_importances_
feature_names = x_train_resampled.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances (Tuned GBC)")
plt.bar(range(x_train_resampled.shape[1]), importances[indices], align="center")
plt.xticks(range(x_train_resampled.shape[1]), feature_names[indices], rotation=90)
plt.xlim([-1, x_train_resampled.shape[1]])
plt.tight_layout()
plt.show()

"""**Reasoning**:
The previous code failed because numpy was not imported. I need to import numpy to use `np.argsort`.


"""

import numpy as np

importances = gbc_tuned.feature_importances_
feature_names = x_train_resampled.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances (Tuned GBC)")
plt.bar(range(x_train_resampled.shape[1]), importances[indices], align="center")
plt.xticks(range(x_train_resampled.shape[1]), feature_names[indices], rotation=90)
plt.xlim([-1, x_train_resampled.shape[1]])
plt.tight_layout()
plt.show()

"""## Save the model

### Subtask:
Save the trained model to a file.

**Reasoning**:
Save the trained Gradient Boosting Classifier model to a file using pickle.
"""

import pickle
filename = 'tuned_diabetes_model.pkl'
pickle.dump(gbc_tuned, open(filename, 'wb'))

"""## Summary:

### Data Analysis Key Findings

*   The dataset initially contained 3854 duplicate rows, which were successfully removed. There were no missing values.
*   The original training data showed a significant class imbalance in the 'diabetes' target variable (value counts were not explicitly shown in the result but the SMOTE application implies this). After applying SMOTE, the training data was balanced, with both classes having the same number of instances.
*   Hyperparameter tuning using `GridSearchCV` identified the best parameters for the Gradient Boosting Classifier as `{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}`.
*   The tuned Gradient Boosting Classifier model achieved a Precision of approximately 0.9241, a Recall of approximately 0.7147, and an F1-score of approximately 0.8060 on the test set.
*   The model demonstrated good discriminative power with an AUC of approximately 0.90.
*   The feature importance plot identified the most important features for predicting diabetes, although the specific feature names and their relative importance values were not explicitly detailed in the result text.
*   The trained model was successfully saved to a file named 'tuned\_diabetes\_model.pkl'.

### Insights or Next Steps

*   The model shows a good balance between precision and recall for predicting diabetes, although there is still room for improvement in recall. Further analysis of false negatives could be beneficial.
*   Investigating the most important features identified by the model could lead to insights into the key risk factors for diabetes in this dataset, potentially informing data collection or feature engineering efforts.

"""